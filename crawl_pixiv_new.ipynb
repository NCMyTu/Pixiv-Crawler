{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e453175-b0bd-4c1d-a5c2-b95dd77ffa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import random\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "HEADERS = {\n",
    "    \"Referer\": \"https://www.pixiv.net/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4b3d6-7ace-4e4e-8216-f4b014528d78",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64576f6c-784d-47e3-a3a3-2c40732a53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_down(driver, n_times):\n",
    "    \"\"\"\n",
    "    Scrolls down the current page.\n",
    "\n",
    "    Parameters:\n",
    "        driver : selenium.webdriver\n",
    "            Selenium WebDriver instance currently viewing the page.\n",
    "        n_times : int\n",
    "            The number of times to scroll down (each scroll is 1000 pixels).\n",
    "    \"\"\"\n",
    "    actions = ActionChains(driver)\n",
    "    \n",
    "    for _ in range(n_times):\n",
    "        actions.scroll_by_amount(0, 5000).perform()  # effectively scroll down to the end of page\n",
    "        time.sleep(random.uniform(0.35, 0.95))\n",
    "\n",
    "def login(driver, email, password):\n",
    "    \"\"\"\n",
    "    Logs in to Pixiv.\n",
    "\n",
    "    Parameters:\n",
    "        driver : selenium.webdriver\n",
    "            Selenium WebDriver instance currently on the Pixiv login page.\n",
    "        email : str\n",
    "            The email address or Pixiv ID used to log in.\n",
    "        password : str\n",
    "            The password corresponding to the given email.\n",
    "    \"\"\"\n",
    "    email_input = driver.find_element(By.CSS_SELECTOR, \"input[placeholder='E-mail address or pixiv ID']\")\n",
    "    email_input.send_keys(email)\n",
    "    time.sleep(1)\n",
    "    password_input = driver.find_element(By.CSS_SELECTOR, \"input[placeholder='Password']\")\n",
    "    password_input.send_keys(password)\n",
    "    time.sleep(1)\n",
    "    password_input.send_keys(Keys.ENTER)\n",
    "\n",
    "def read_account_info(path):\n",
    "    \"\"\"\n",
    "    Reads account login information from a file.\n",
    "\n",
    "    Parameters:\n",
    "        path : str\n",
    "            The file path to a text file containing login information in key=value format.\n",
    "\n",
    "    Returns:\n",
    "        info : dict{str: str}\n",
    "            A dictionary where each key is the left-hand side of a line split by \"=\", \n",
    "            and the corresponding right-hand side value\n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            temp = line.split(\"=\")\n",
    "            key = temp[0].strip()\n",
    "            value = temp[1].strip()\n",
    "\n",
    "            info[key] = value\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e5e68d-7dc5-430c-9afa-6504df7ce491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_artwork_ids(driver):\n",
    "    \"\"\"\n",
    "    Extracts artwork IDs from the current Pixiv page.\n",
    "\n",
    "    Parameters:\n",
    "        driver : selenium.webdriver\n",
    "            Selenium WebDriver instance currently pointing to a Pixiv page with artworks.\n",
    "\n",
    "    Returns:\n",
    "        ids: list[int]\n",
    "            A list of artwork IDs extracted from the page.\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "\n",
    "    artworks = driver.find_elements(By.XPATH, \"//a[contains(@href, '/en/artworks/')]\")\n",
    "    \n",
    "    for artwork in artworks:\n",
    "        id = artwork.get_attribute(\"data-gtm-value\")\n",
    "        if id is not None:\n",
    "            ids.append(int(id))\n",
    "\n",
    "    return ids\n",
    "    \n",
    "def get_img_urls_from_artwork_id(driver, artwork_id, has_gui=False):\n",
    "    \"\"\"\n",
    "    Retrieves all image URLs from a Pixiv artwork page from its artwork ID.\n",
    "\n",
    "    Parameters:\n",
    "        driver : selenium.webdriver\n",
    "            Selenium WebDriver instance used to navigate and scrape the page.\n",
    "        artwork_id : int or str\n",
    "            The ID of the artwork to retrieve image URLs from.\n",
    "        has_gui : bool, default=False\n",
    "            Not implemented, indicates if the driver is running with a GUI.\n",
    "\n",
    "    Returns:\n",
    "        bool\n",
    "            Indicates images found (True) or animated/ugoira (False).\n",
    "        list[str]\n",
    "            A list of image URLs. Will be empty if the artwork is animated or an error occurs.\n",
    "    \"\"\"\n",
    "    artwork_url = f\"https://www.pixiv.net/en/artworks/{artwork_id}\"\n",
    "    img_urls = []\n",
    "    \n",
    "    # show_all_class = \"sc-e1dc2ae6-1 fUQgzA\"\n",
    "    # reading_works_class = \"sc-e1dc2ae6-1 fUQgzA\" #\"sc-13c1e204-0 ixmPpS\"\n",
    "    img_classes = [\"sc-9ad52c08-1 feuJAv\", \"sc-9ad52c08-1 bZPWaJ\"]\n",
    "    has_multiple_pages = True\n",
    "\n",
    "    driver.get(artwork_url)\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    show_button_class = \"sc-263d9368-5-button iKWUyZ\"\n",
    "    try:\n",
    "        show_button = driver.find_element(By.XPATH, f'//button[@class=\"{show_button_class}\"]')\n",
    "        show_button.click()\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "        \n",
    "    # find animated/ugoira. we don't download this\n",
    "    try:\n",
    "        canvas = driver.find_element(By.TAG_NAME, \"canvas\")\n",
    "        return False, []\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "    # set correct <img> class\n",
    "    multi_page_indicator_class = \"sc-9222a8f6-2 kufPoS\"\n",
    "    try:\n",
    "        # find \"Show all\" or \"Reading works\" div\n",
    "        div = driver.find_element(By.XPATH, f'//div[@class=\"{multi_page_indicator_class}\"]')\n",
    "        # if div.text == \"Reading works\":\n",
    "        #     img_class = reading_works_class\n",
    "        # elif div.text == \"Show all\":\n",
    "        #     img_class = show_all_class\n",
    "    except NoSuchElementException:\n",
    "        # img_class = show_all_class\n",
    "        has_multiple_pages = False\n",
    "    \n",
    "    # print(f\"img_class: {img_class}\")\n",
    "    \n",
    "    # find number of pages\n",
    "    if has_multiple_pages:\n",
    "        n_page_class = \"sc-b5e6ab10-0 krtyqW\"\n",
    "        n_pages_div = driver.find_elements(By.XPATH, f'//div[@class=\"{n_page_class}\"]')[0]\n",
    "        span = n_pages_div.find_element(By.TAG_NAME, 'span')\n",
    "        n_pages = int(span.text.strip().split(\"/\")[-1])\n",
    "    else:\n",
    "        n_pages = 1\n",
    "    \n",
    "    # print(f\"n_pages: {n_pages}\")\n",
    "\n",
    "    # get image base URL\n",
    "    img_element = None\n",
    "    \n",
    "    for img_class in img_classes:\n",
    "        try:\n",
    "            img_element = driver.find_element(By.XPATH, f'//img[@class=\"{img_class}\"]')\n",
    "            break\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "\n",
    "    if img_element is None:\n",
    "        raise Exception(\"No image with specified classes was found.\")\n",
    "    else:    \n",
    "        base_img_url = img_element.get_attribute(\"src\").split(\"_\")[0]\n",
    "\n",
    "    # construct all images' URL\n",
    "    for i in range(n_pages):\n",
    "        img_url = base_img_url + f\"_p{i}_master1200.jpg\"\n",
    "        img_urls.append(img_url)\n",
    "\n",
    "    return True, img_urls\n",
    "\n",
    "def get_img_and_pg_num(img_url):\n",
    "    \"\"\"\n",
    "    Extracts the artwork ID and page number from a Pixiv image URL.\n",
    "\n",
    "    Parameters:\n",
    "        img_url : str\n",
    "            The direct URL of the Pixiv image.\n",
    "\n",
    "    Returns:\n",
    "        artwork_id : int\n",
    "            Artwork's ID extracted from the URL.\n",
    "        page_number : int\n",
    "            Page number extracted from the URL.\n",
    "    \"\"\"\n",
    "    # image's url is in format:\n",
    "    # https://i.pximg.net/img-master/img/{yyyy}/{mm}/{dd}/{hh}/{mm}/{ss}/{artwork_id}_p{page_number}_master1200.jpg\n",
    "\n",
    "    img_info = img_url.split(\"/\")[-1]\n",
    "    img_info = img_info.split(\"_\")\n",
    "\n",
    "    artwork_id = int(img_info[0])\n",
    "    page_number = int(img_info[1][1:])\n",
    "    \n",
    "    return artwork_id, page_number\n",
    "\n",
    "def construct_file_name(url):\n",
    "    \"\"\"\n",
    "    Constructs a file name from a Pixiv image URL.\n",
    "\n",
    "    Parameters:\n",
    "        url : str\n",
    "            The direct URL of a Pixiv image.\n",
    "\n",
    "    Returns:\n",
    "        file_name : str\n",
    "            File name in the format \"{artwork_id}_p{page_number}.jpg\".\n",
    "    \"\"\"\n",
    "    artwork_id, page_number = get_img_and_pg_num(url)\n",
    "    file_name = f\"{artwork_id}_p{page_number}.jpg\"\n",
    "    return file_name\n",
    "    \n",
    "def download_img(save_path, url, timeout=10):\n",
    "    \"\"\"\n",
    "    Downloads an image from a given URL and saves it.\n",
    "\n",
    "    Parameters:\n",
    "        save_path : str\n",
    "            The file path where the image will be saved.\n",
    "        url : str\n",
    "            The URL of the image.\n",
    "        timeout : int, default=10\n",
    "            Maximum seconds to wait for the response.\n",
    "\n",
    "    Returns:\n",
    "        is_success : bool\n",
    "            True if the download is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return False\n",
    "            \n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "            return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return False\n",
    "\n",
    "def THREAD_download_img(save_path, url, timeout=10):\n",
    "    \"\"\"\n",
    "    Wrapper function to download a Pixiv image from an URL.\n",
    "    Only use this function in multi-threaded execution.\n",
    "\n",
    "    Parameters:\n",
    "        save_path : str\n",
    "            Directory where the image should be saved.\n",
    "        url : str\n",
    "            The direct URL of the Pixiv image.\n",
    "        timeout : int, default=10\n",
    "            Maximum seconds to wait for the response.\n",
    "\n",
    "    Returns:\n",
    "        is_success : bool\n",
    "            True if the image was downloaded successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    file_name = construct_file_name(url)\n",
    "    file_path = os.path.join(save_path, file_name)\n",
    "\n",
    "    return download_img(file_path, url, timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa4a9d-bb0c-431a-a55f-e33b2c8c5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_failed_info(failed_list, n_ids_per_row=5):\n",
    "    if len(failed_download_urls) > 0:\n",
    "        print(f\"{len(failed_download_urls)} failed:\")\n",
    "    \n",
    "    for i in range(0, len(failed_download_urls), n_ids_per_row):\n",
    "        for j in range(i, min(i + n_ids_per_row, len(failed_download_urls))):\n",
    "            row = failed_download_urls[i:i + n_ids_per_row]\n",
    "            row_str = \", \".join(str(_id) for _id in row)\n",
    "        print(f\"\\t\\t{row_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2485f9-1085-45ea-806e-df32b224abe7",
   "metadata": {},
   "source": [
    "# Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dcfad-509d-490b-92fa-9b84b65f0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = r\"\"\n",
    "\n",
    "account_info = read_account_info(\"account.txt\")\n",
    "email = account_info[\"email\"]\n",
    "password = account_info[\"password\"]\n",
    "nickname = account_info[\"nickname\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe8b03-a896-4242-b864-e53dd95e2ace",
   "metadata": {},
   "source": [
    "### Collect image URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df202c5d-1291-407b-af2b-6d2c21f501d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_id = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2585520-c914-42cd-a066-0024a30a8e9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_page = 1\n",
    "max_page = None\n",
    "\n",
    "# construct artist page\n",
    "artist_page = f\"https://www.pixiv.net/en/users/{artist_id}/artworks/?p={{}}\"\n",
    "\n",
    "artwork_ids = []\n",
    "\n",
    "# set up Chrome WebDriver\n",
    "driver = uc.Chrome(use_subprocess=False)\n",
    "driver.maximize_window()\n",
    "\n",
    "login_page = \"https://accounts.pixiv.net/login\"\n",
    "driver.get(login_page)\n",
    "\n",
    "# log in\n",
    "print(\"Log in... START\")\n",
    "# find email and password field\n",
    "login(driver, email, password)\n",
    "print(\"Log in... DONE\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# find all artwork's ids\n",
    "print(\"Find all artwork's ids ... START\")\n",
    "has_next_page = True\n",
    "\n",
    "while has_next_page:\n",
    "    print(f\"Page {current_page}: \", end=\"\")\n",
    "\n",
    "    current_artist_page = artist_page.format(current_page)\n",
    "    \n",
    "    driver.get(current_artist_page)\n",
    "    time.sleep(random.uniform(0.5, 1.5))\n",
    "    scroll_down(driver, n_times=10)\n",
    "    \n",
    "    current_page_artwork_ids = get_artwork_ids(driver)\n",
    "\n",
    "    print(f\"found {len(current_page_artwork_ids)} artworks:\\n{\", \".join(map(str, current_page_artwork_ids))}\")\n",
    "\n",
    "    artwork_ids.extend(current_page_artwork_ids)\n",
    "    \n",
    "    if max_page is not None and current_page >= max_page:\n",
    "        break\n",
    "        \n",
    "    # check if next page exists\n",
    "    # find previous and next button\n",
    "    next_page_class = \"sc-27a0ff07-2 jGoxAA sc-27a0ff07-1-filterProps-Styled-Component dDrHMO\"\n",
    "    btns = driver.find_elements(By.XPATH, f'//a[contains(@class, \"{next_page_class}\")]')\n",
    "    hidden_attr = btns[-1].get_attribute(\"hidden\")\n",
    "\n",
    "    # no hidden attribute = next page is available\n",
    "    has_next_page = (hidden_attr is None)\n",
    "\n",
    "    current_page += 1\n",
    "    \n",
    "print(\"Find all artwork's ids... DONE\\nCollecting image URLs... START\")\n",
    "\n",
    "# collect image urls\n",
    "animated_artwork_ids = []\n",
    "img_urls = []\n",
    "\n",
    "for artwork_id in artwork_ids:\n",
    "    is_image, artwork_img_urls = get_img_urls_from_artwork_id(driver, artwork_id)\n",
    "\n",
    "    if is_image:\n",
    "        img_urls.extend(artwork_img_urls)\n",
    "    else:\n",
    "        animated_artwork_ids.append(artwork_id)\n",
    "\n",
    "    if random.random() < 0.1:\n",
    "        time.sleep(random.uniform(1.5, 3))\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# print crawled information\n",
    "print(f\"Collecting image URLs... DONE\\nFound a total of {len(artwork_ids)} artworks, which includes:\")\n",
    "\n",
    "print(f\"\\t{len(animated_artwork_ids)} animated/uroiga artworks:\")\n",
    "if len(animated_artwork_ids) > 0:\n",
    "    n_ids_per_row = 5\n",
    "    for i in range(0, len(animated_artwork_ids), n_ids_per_row):\n",
    "        for j in range(i, min(i + n_ids_per_row, len(animated_artwork_ids))):\n",
    "            row = animated_artwork_ids[i:i + n_ids_per_row]\n",
    "            row_str = \", \".join(str(_id) for _id in row)\n",
    "        print(f\"\\t\\t{row_str}\")\n",
    "\n",
    "print(f\"\\t{len(img_urls)} image URLs:\")\n",
    "for url in img_urls:\n",
    "    print(\"\\t\", \"\\t\", url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c655879-e350-4918-9dea-d63a13cff60a",
   "metadata": {},
   "source": [
    "### Download (multi-threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb22b5-2199-4f07-8d54-36d05dcb828f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_workers = 30\n",
    "count = 1\n",
    "\n",
    "failed_download_urls = []\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    results = {executor.submit(THREAD_download_img, SAVE_PATH, url, 7): construct_file_name(url) for url in img_urls}\n",
    "    \n",
    "    for future in concurrent.futures.as_completed(results):\n",
    "        is_success = future.result()\n",
    "        file_name = results[future]\n",
    "\n",
    "        line = f\"[{count}/{len(img_urls)}] {file_name}\"\n",
    "        print(f\"{line:<35}\", end=\"\")\n",
    "        if is_success:\n",
    "            print(\"ok\")\n",
    "        else:\n",
    "            print(\"FAILED\")\n",
    "            failed_download_urls.append(file_name.split(\".\")[0])\n",
    "\n",
    "        count += 1\n",
    "\n",
    "print(f\"Download in {time.time() - start:.4f}s\")\n",
    "    \n",
    "print_failed_info(failed_download_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772304c2-d5b5-4e13-8de3-d0b8eb10dd9d",
   "metadata": {},
   "source": [
    "### Download (single-threaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca345495-9698-403e-8ff7-a0ff62f878c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "failed_download_urls = []\n",
    "\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, url in enumerate(img_urls):\n",
    "    file_name = construct_file_name(url)\n",
    "    file_path = os.path.join(SAVE_PATH, file_name)\n",
    "\n",
    "    print(f\"[{i+1}/{len(img_urls)}] downloading {file_name}\\t\\t\", end=\"\")\n",
    "\n",
    "    is_success = download_img(file_path, url, timeout=10)\n",
    "    \n",
    "    if is_success:\n",
    "        print(\"ok\")\n",
    "    else:\n",
    "        print(\"FAILED\")\n",
    "        failed_download_urls.append(url)\n",
    "\n",
    "    time.sleep(random.choice([0, 0.5]))\n",
    "\n",
    "print(f\"Download in {time.time() - start:.4f}s\")\n",
    "\n",
    "print_failed_info(failed_download_urls)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
